Python 3.7.6 (default, Jan  8 2020, 20:23:39) [MSC v.1916 64 bit (AMD64)]
Type "copyright", "credits" or "license" for more information.

IPython 7.13.0 -- An enhanced Interactive Python.

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
merc_test = pd.read_csv("test.csv")
merc_train = pd.read_csv("train.csv")


plt.scatter(range(len(merc_train)), np.sort(merc_train.y.values), alpha=0.3)
Out[2]: <matplotlib.collections.PathCollection at 0x2399808ad48>


Figures now render in the Plots pane by default. To make them also appear inline in the Console, uncheck "Mute Inline Plotting" under the Plots pane options menu. 


 

y_train = merc_train.iloc[:,1].values
x_train = merc_train.iloc[:,2:378].values

x_test = merc_test.iloc[:,1:377].values


from sklearn.preprocessing import LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder


i = 0
while i <= 9:
    le_train = LabelEncoder()
    le_train.fit_transform(x_train[:,i])
    x_train[:,i]=le_train.fit_transform(x_train[:,i])
    i +=1

j = 0
while j <= 8:
    le_test = LabelEncoder()
    le_test.fit_transform(x_test[:,j])
    x_test[:,j]=le_test.fit_transform(x_test[:,j])
    j +=1


x1_train = pd.DataFrame(x_train)
x1_test = pd.DataFrame(x_test)
v_test = x1_test.var()
v_train = x1_train.var()
y1_train = pd.DataFrame(y_train)
vy_train = y1_train.var()

# Adding Variance row
x1_test = x1_test.append(v_test, ignore_index = True)
x1_train= x1_train.append(v_train, ignore_index = True)

#Dropping columns with zero variance
x2_train=x1_train.drop(x1_train.columns[x1_train.loc[4209,:] == 0],axis=1)
x2_test=x1_test.drop(x1_test.columns[x1_test.loc[4209,:] == 0],axis=1)


x1_train.isnull().any().any()
x1_test.isnull().any().any()
y1_train.isnull().any().any()
x1_train.nunique(dropna=True)
x1_test.nunique(dropna=True)
y1_train.nunique(dropna=True)
Out[6]: 
0    2545
dtype: int64


from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
x1_test = sc.fit_transform(x1_test)
x1_train = sc.fit_transform(x1_train)
y1_train = sc.fit_transform(y1_train)


from sklearn.decomposition import PCA
pca = PCA(n_components = None)
x_test_pca = pca.fit_transform(x1_test)
x_train_pca = pca.transform(x1_train)
explained_variance = pca.explained_variance_ratio_



from sklearn.model_selection import train_test_split
xg_train, xg_cv, y_xg_train, y_xg_cv = train_test_split(x_train, y_train, test_size=0.3,random_state = 0)



import xgboost as xgb
from xgboost import XGBRegressor
Reg = XGBRegressor()
Reg.fit(xg_train,y_xg_train)
[15:40:56] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
Out[11]: 
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0,
             importance_type='gain', learning_rate=0.1, max_delta_step=0,
             max_depth=3, min_child_weight=1, missing=None, n_estimators=100,
             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,
             silent=None, subsample=1, verbosity=1)


y_cvpred = Reg.predict(xg_cv)
y_test = Reg.predict(x_test)

plt.scatter(range(len(xg_cv)), np.sort(y_cvpred), alpha=0.3)
Out[13]: <matplotlib.collections.PathCollection at 0x2399b42c948>

from sklearn.metrics import r2_score
print(r2_score(y_xg_cv, y_cvpred))
0.5167053132097317
